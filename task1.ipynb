{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from pyspark.sql.functions import split, col, regexp_replace, collect_list, explode, concat, collect_set, array_union, flatten\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, StringType, BooleanType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/25 12:56:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://macbook-pro.home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Projet-Task-1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x172a778d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Projet-Task-1\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4G\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(\"data.txt\").toDF(\"Log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Log: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- First Server: string (nullable = true)\n",
      " |-- Second Server: string (nullable = true)\n",
      " |-- Communication Type: string (nullable = true)\n",
      " |-- Process ID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Log\", regexp_replace(col(\"Log\"), \"[<>]\", \"\"))\n",
    "df = df.withColumn(\"Log\", regexp_replace(col(\"Log\"), \",\", \"\"))\n",
    "df = df.withColumn(\"Log\", split(col(\"Log\"), \" \"))\n",
    "\n",
    "# df.printSchema()\n",
    "# df.show()\n",
    "\n",
    "columns = [\"First Server\", \"Second Server\", \"Communication Type\", \"Process ID\"]\n",
    "\n",
    "for i in range(len(columns)):\n",
    "    # print(columns[i])\n",
    "    df = df.withColumn(columns[i], col(\"Log\")[i])\n",
    "\n",
    "df.printSchema()\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Process ID from request \n",
    "log = udf(lambda x: x[:-1], ArrayType(StringType())) \n",
    "df = df.withColumn('Log', log('Log')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Process ID: string (nullable = true)\n",
      " |-- Log: array (nullable = false)\n",
      " |    |-- element: array (containsNull = false)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- First Server: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- Second Server: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- FCommunication Type: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_df = df.groupBy(\"Process ID\").agg(collect_list(\"Log\").alias(\"Log\"), \n",
    "                                          collect_list(\"First Server\").alias(\"First Server\"),\n",
    "                                          collect_list(\"Second Server\").alias(\"Second Server\"),\n",
    "                                          collect_list(\"Communication Type\").alias(\"FCommunication Type\"))\n",
    "\n",
    "grouped_df.printSchema()\n",
    "# grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Process ID: string (nullable = true)\n",
      " |-- First Server: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- Second Server: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- Servers: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distinct_servers_df = df.groupBy(\"Process ID\").agg(collect_set(\"First Server\").alias(\"First Server\"),\n",
    "                                                   collect_set(\"Second Server\").alias(\"Second Server\"))\n",
    "\n",
    "distinct_servers_df = distinct_servers_df.withColumn(\"Servers\", array_union(\"First Server\", \"Second Server\"))\n",
    "\n",
    "distinct_servers_df.printSchema()\n",
    "# distinct_servers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Process ID: string (nullable = true)\n",
      " |-- Characteristic Matrix: vector (nullable = true)\n",
      "\n",
      "Rows of Characteristic Matrix:  ['null', 'S-1', 'S-12.2', 'S-12.1', 'S-7.1', 'S-7.2', 'S-9', 'S-14', 'S-17', 'S-11', 'S-15', 'S-8', 'S-16', 'S-5', 'S-6', 'S-3', 'S-13', 'S-10', 'S-18.2', 'S-4', 'S-2', 'S-18.1', 'S-19']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "characteristics = CountVectorizer(inputCol=\"Servers\", outputCol=\"Characteristic Matrix\")\n",
    "\n",
    "model = characteristics.fit(distinct_servers_df)\n",
    "char_matrix = model.transform(distinct_servers_df).select(\"Process ID\", \"Characteristic Matrix\")\n",
    "\n",
    "char_matrix.printSchema()\n",
    "# char_matrix.show()\n",
    "\n",
    "servers = model.vocabulary\n",
    "print(\"Rows of Characteristic Matrix: \", servers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "minhash = MinHashLSH(inputCol=\"Characteristic Matrix\", outputCol=\"Signatures\", numHashTables=5)\n",
    "\n",
    "# MinHash produces the signatures for the Characteritic matrix \n",
    "# numvHashTables is the number of the hush functioms that we want to use and the lenght of the signature \n",
    "model = minhash.fit(char_matrix)\n",
    "signatures = model.transform(char_matrix)\n",
    "\n",
    "# signatures.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approxSimilarityJoin uses autmatically LSH to find rows that it is most likely \n",
    "# to have same \"Signatures\"\n",
    "# threshold: pairs with Jaccard Distance lower than threshlod\n",
    "similar_pairs = model.approxSimilarityJoin(signatures, signatures, threshold=0.01, distCol=\"Jaccard Distance\")\n",
    "# similar_pairs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar_pairs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_pairs = similar_pairs.select(\"datasetA.Process ID\", \"datasetB.Process ID\", \n",
    "                     \"Jaccard Distance\") \\\n",
    "                    # .filter((col(\"datasetA.Process ID\") != col(\"datasetB.Process ID\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = [\"Process ID A\", \"Process ID B\", \"Jaccard Distance\"]\n",
    "similar_pairs = similar_pairs.toDF(*new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar_pairs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = similar_pairs.join(grouped_df, similar_pairs[\"Process ID A\"] == col(\"Process ID\")) \\\n",
    "                     .select(col(\"Process ID A\"), col(\"Process ID B\"), col(\"Log\").alias(\"Log A\")) \\\n",
    "                     .join(grouped_df, similar_pairs[\"Process ID B\"] == col(\"Process ID\")) \\\n",
    "                     .select(col(\"Process ID A\"), col(\"Process ID B\"), col(\"Log A\"), col(\"Log\").alias(\"Log B\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_check(x,y):\n",
    "    return x==y\n",
    "\n",
    "orifinal_checking = udf(original_check, BooleanType())\n",
    "same_pairs = pairs.filter(orifinal_checking(col(\"Log A\"), col(\"Log B\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "same_pairs = same_pairs.groupBy(\"Log A\").agg(collect_set(\"Process ID A\").alias(\"Process Set\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Log A: array (nullable = false)\n",
      " |    |-- element: array (containsNull = false)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- Process Set: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "same_pairs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output - part1Observations.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/25 12:56:45 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "same_pairs_col = same_pairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"part1Observations.txt\", \"w\") as file:\n",
    "    current_group = None\n",
    "    for row in same_pairs_col:\n",
    "        process_set = row[\"Process Set\"]\n",
    "        # process_id = row[\"Process ID\"]\n",
    "        log = row[\"Log A\"]\n",
    "        \n",
    "        if process_set != current_group:\n",
    "            file.write(\"\\n\")\n",
    "            process_set_string = ', '.join(str(x) for x in process_set)\n",
    "            file.write(f\"Group: {{{process_set_string}}}\\n\")\n",
    "            current_group = process_set\n",
    "        \n",
    "        for process in process_set:\n",
    "            file.write(f\"\\n{process}:\\n\")\n",
    "            for l in log:\n",
    "                log_concat = ', '.join(str(x) for x in l)\n",
    "                file.write(f\"<{log_concat}>\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output - part1Output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Log', 'array<string>'),\n",
       " ('First Server', 'string'),\n",
       " ('Second Server', 'string'),\n",
       " ('Communication Type', 'string'),\n",
       " ('Process ID', 'string')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df = df.withColumn('pid_integer', df['Process ID'].cast(IntegerType()))\n",
    "max_process_id = df.agg({\"pid_integer\": \"max\"}).collect()[0][0]\n",
    "# print(max_process_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"part1Output.txt\", \"w\") as file:\n",
    "    current_group = None\n",
    "    process_id = max_process_id+1\n",
    "\n",
    "    for row in same_pairs_col:\n",
    "        log = row[\"Log A\"]\n",
    "        \n",
    "        file.write(f\"\\n{process_id}:\\n\")\n",
    "        for l in log:\n",
    "            log_concat = ', '.join(str(x) for x in l)\n",
    "            log_concat = f\"{log_concat}, {process_id}\"\n",
    "            file.write(f\"<{log_concat}>\\n\")\n",
    "        process_id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shingling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_shingling(text, k):\n",
    "    shingles = set()\n",
    "    for i in range(len(text) - k + 1):\n",
    "        shingle = text[i:i + k]\n",
    "        shingles.add(shingle)\n",
    "    return list(shingles)\n",
    "\n",
    "k_shingling_udf = udf(lambda text: k_shingling(text, 5), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(\"data.txt\").toDF(\"Log\")\n",
    "log = udf(lambda x: x[1:-4], StringType()) \n",
    "df = df.withColumn('Log_split', log('Log')) \n",
    "df.collect()\n",
    "df_shingles = df.withColumn(\"Shingles\", k_shingling_udf(df[\"Log_split\"]))\n",
    "# df_shingles.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_shingles.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Log: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- Log_split: string (nullable = true)\n",
      " |-- Shingles: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "First Server\n",
      "Second Server\n",
      "Communication Type\n",
      "Process ID\n",
      "root\n",
      " |-- Log: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- Log_split: string (nullable = true)\n",
      " |-- Shingles: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- First Server: string (nullable = true)\n",
      " |-- Second Server: string (nullable = true)\n",
      " |-- Communication Type: string (nullable = true)\n",
      " |-- Process ID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_shingles = df_shingles.withColumn(\"Log\", regexp_replace(col(\"Log\"), \"[<>]\", \"\"))\n",
    "df_shingles = df_shingles.withColumn(\"Log\", regexp_replace(col(\"Log\"), \",\", \"\"))\n",
    "df_shingles = df_shingles.withColumn(\"Log\", split(col(\"Log\"), \" \"))\n",
    "\n",
    "df_shingles.printSchema()\n",
    "# df_shingles.show()\n",
    "\n",
    "columns = [\"First Server\", \"Second Server\", \"Communication Type\", \"Process ID\"]\n",
    "\n",
    "for i in range(len(columns)):\n",
    "    print(columns[i])\n",
    "    df_shingles = df_shingles.withColumn(columns[i], col(\"Log\")[i])\n",
    "\n",
    "df_shingles.printSchema()\n",
    "# df_shingles.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = udf(lambda x: x[:-1], ArrayType(StringType())) \n",
    "df_shingles = df_shingles.withColumn('Log', log('Log')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Process ID: string (nullable = true)\n",
      " |-- Shingles: array (nullable = false)\n",
      " |    |-- element: array (containsNull = false)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- Log: array (nullable = false)\n",
      " |    |-- element: array (containsNull = false)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- Flat shingles: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_df = df_shingles.groupBy(\"Process ID\").agg(collect_set(\"Shingles\").alias(\"Shingles\"),collect_list(\"Log\").alias(\"Log\"))\n",
    "grouped_df = grouped_df.withColumn(\"Flat shingles\", flatten(col(\"Shingles\")))\n",
    "grouped_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Process ID: string (nullable = true)\n",
      " |-- Characteristic Matrix: vector (nullable = true)\n",
      "\n",
      "Rows of Characteristic Matrix:  ['S-1, ', ', S-1', 'espon', ' Resp', 'Respo', ' Requ', 'ponse', 'eques', ', Res', 'quest', ', Req', 'Reque', 'spons', 'onse,', 'uest,', 'est, ', 'nse, ', '1, Re', 'null,', 'ull, ', '-1, R', ' S-1,', '1, S-', 'S-12.', 'l, Re', 'll, R', ' null', ', nul', '1, nu', 'l, S-', '-1, n', 'll, S', '2, S-', '2, Re', '.2, R', '.2, S', '-1, S', '12.2,', '-12.2', '2.2, ', ' S-12', '.1, S', '.1, R', '-12.1', '12.1,', '2.1, ', '7.1, ', 'S-7.1', '-7.1,', ', S-7', ' S-7.', '7.2, ', '-7.2,', 'S-7.2', 'S-9, ', 'S-14,', '-14, ', '-17, ', 'S-17,', 'S-11,', '-11, ', 'st, 7', 'se, 7', 'se, 5', 'st, 5', 'st, 8', 'se, 8', 'st, 1', 'se, 1', 'st, 2', 'se, 2', 'se, 3', 'st, 3', 'st, 6', 'st, 9', 'se, 9', 'se, 6', 'se, 4', 'st, 4', '-15, ', 'S-15,', '9, S-', '9, Re', '4, Re', '4, S-', '-9, S', '-9, R', ' S-9,', ', S-9', '14, R', '14, S', ' S-14', '5, S-', '5, Re', 'S-8, ', '-16, ', 'S-16,', '6, Re', '6, S-', 'S-5, ', '7, S-', ' S-17', '17, R', '17, S', '7, Re', '11, R', ' S-11', '11, S', ' S-15', '15, S', '15, R', 'S-6, ', '-8, R', '-8, S', '8, Re', ', S-8', '8, S-', ' S-8,', '16, S', '16, R', ' S-16', 'S-18.', 'S-3, ', '-5, R', ', S-5', '-5, S', ' S-5,', '3, Re', '3, S-', '-6, R', '-6, S', ' S-6,', ', S-6', '-13, ', 'S-13,', '-10, ', 'S-10,', '-18.2', '18.2,', '8.2, ', ' S-18', 'S-4, ', '-3, R', '-3, S', ', S-3', ' S-3,', 'S-2, ', '18.1,', '-18.1', '8.1, ', '13, S', '13, R', ' S-13', '0, Re', '10, R', '10, S', ' S-10', '0, S-', ' S-4,', '-4, R', ', S-4', '-4, S', '-2, S', '-2, R', ', S-2', ' S-2,', 'S-19,', '-19, ', '19, S', '19, R', ' S-19']\n"
     ]
    }
   ],
   "source": [
    "characteristics = CountVectorizer(inputCol=\"Flat shingles\", outputCol=\"Characteristic Matrix\")\n",
    "\n",
    "model = characteristics.fit(grouped_df)\n",
    "char_matrix = model.transform(grouped_df).select(\"Process ID\", \"Characteristic Matrix\")\n",
    "\n",
    "char_matrix.printSchema()\n",
    "# char_matrix.show()\n",
    "\n",
    "shingles = model.vocabulary\n",
    "print(\"Rows of Characteristic Matrix: \", shingles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "minhash = MinHashLSH(inputCol=\"Characteristic Matrix\", outputCol=\"Signatures\", numHashTables=5)\n",
    "\n",
    "# MinHash produces the signatures for the Characteritic matrix \n",
    "# numvHashTables is the number of the hush functioms that we want to use and the lenght of the signature \n",
    "model = minhash.fit(char_matrix)\n",
    "signatures = model.transform(char_matrix)\n",
    "\n",
    "# signatures.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approxSimilarityJoin uses autmatically LSH to find rows that it is most likely \n",
    "# to have same \"Signatures\"\n",
    "# threshold: pairs with Jaccard Distance lower than threshlod\n",
    "similar_pairs = model.approxSimilarityJoin(signatures, signatures, threshold=0.2, distCol=\"Jaccard Distance\")\n",
    "# similar_pairs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar_pairs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_pairs = similar_pairs.select(\"datasetA.Process ID\", \"datasetB.Process ID\", \n",
    "                     \"Jaccard Distance\")\\\n",
    "                        # .filter((col(\"datasetA.Process ID\") != col(\"datasetB.Process ID\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar_pairs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = [\"Process ID A\", \"Process ID B\", \"Jaccard Distance\"]\n",
    "similar_pairs = similar_pairs.toDF(*new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar_pairs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = similar_pairs.join(grouped_df, similar_pairs[\"Process ID A\"] == col(\"Process ID\")) \\\n",
    "                     .select(col(\"Process ID A\"), col(\"Process ID B\"), col(\"Log\").alias(\"Log A\")) \\\n",
    "                     .join(grouped_df, similar_pairs[\"Process ID B\"] == col(\"Process ID\")) \\\n",
    "                     .select(col(\"Process ID A\"), col(\"Process ID B\"), col(\"Log A\"), col(\"Log\").alias(\"Log B\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_check(x,y):\n",
    "    return x==y\n",
    "\n",
    "orifinal_checking = udf(original_check, BooleanType())\n",
    "same_pairs = pairs.filter(orifinal_checking(col(\"Log A\"), col(\"Log B\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_pairs = same_pairs.groupBy(\"Log A\").agg(collect_set(\"Process ID A\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same_pairs.select(\"collect_set(Process ID A)\").collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_intensive_systems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
