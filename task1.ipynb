{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from pyspark.sql.functions import split, col, regexp_replace, collect_list, explode, concat, collect_set, array_union, flatten\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, StringType, BooleanType\n",
    "\n",
    "import time\n",
    "import random\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/25 23:55:13 WARN Utils: Your hostname, abha-ThinkPad-P14s-Gen-4 resolves to a loopback address: 127.0.1.1; using 192.168.178.94 instead (on interface wlp2s0)\n",
      "24/06/25 23:55:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/25 23:55:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.178.94:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Projet-Task-1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7bcf1cdf6750>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Projet-Task-1\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"12G\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"6G\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"2G\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:MaxGCPauseMillis=500 -XX:InitiatingHeapOccupancyPercent=35\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC -XX:MaxGCPauseMillis=500 -XX:InitiatingHeapOccupancyPercent=35\") \\\n",
    "    .getOrCreate()\n",
    "# spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", partition)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data and Clean Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(\"data.txt\").toDF(\"Log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Log: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- First Server: string (nullable = true)\n",
      " |-- Second Server: string (nullable = true)\n",
      " |-- Communication Type: string (nullable = true)\n",
      " |-- Process ID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Log\", regexp_replace(col(\"Log\"), \"[<>]\", \"\"))\n",
    "df = df.withColumn(\"Log\", regexp_replace(col(\"Log\"), \",\", \"\"))\n",
    "df = df.withColumn(\"Log\", split(col(\"Log\"), \" \"))\n",
    "\n",
    "columns = [\"First Server\", \"Second Server\", \"Communication Type\", \"Process ID\"]\n",
    "\n",
    "for i in range(len(columns)):\n",
    "    # print(columns[i])\n",
    "    df = df.withColumn(columns[i], col(\"Log\")[i])\n",
    "\n",
    "df.printSchema()\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Process ID from request \n",
    "log = udf(lambda x: x[:-1], ArrayType(StringType())) \n",
    "df = df.withColumn('Log', log('Log')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Log Events by Process ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Process ID: string (nullable = true)\n",
      " |-- Log: array (nullable = false)\n",
      " |    |-- element: array (containsNull = false)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- First Server: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- Second Server: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- FCommunication Type: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_df = df.groupBy(\"Process ID\").agg(collect_list(\"Log\").alias(\"Log\"), \n",
    "                                          collect_list(\"First Server\").alias(\"First Server\"),\n",
    "                                          collect_list(\"Second Server\").alias(\"Second Server\"),\n",
    "                                          collect_list(\"Communication Type\").alias(\"FCommunication Type\"))\n",
    "\n",
    "grouped_df.printSchema()\n",
    "# grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Process ID: string (nullable = true)\n",
      " |-- First Server: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- Second Server: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- Servers: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distinct_servers_df = df.groupBy(\"Process ID\").agg(collect_set(\"First Server\").alias(\"First Server\"),\n",
    "                                                   collect_set(\"Second Server\").alias(\"Second Server\"))\n",
    "\n",
    "distinct_servers_df = distinct_servers_df.withColumn(\"Servers\", array_union(\"First Server\", \"Second Server\"))\n",
    "\n",
    "distinct_servers_df.printSchema()\n",
    "# distinct_servers_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Characteristic Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Process ID: string (nullable = true)\n",
      " |-- Characteristic Matrix: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "characteristics = CountVectorizer(inputCol=\"Servers\", outputCol=\"Characteristic Matrix\")\n",
    "\n",
    "model = characteristics.fit(distinct_servers_df)\n",
    "char_matrix = model.transform(distinct_servers_df).select(\"Process ID\", \"Characteristic Matrix\")\n",
    "\n",
    "char_matrix.printSchema()\n",
    "# char_matrix.show()\n",
    "\n",
    "servers = model.vocabulary\n",
    "# print(\"Rows of Characteristic Matrix: \", servers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate MinHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "minhash = MinHashLSH(inputCol=\"Characteristic Matrix\", outputCol=\"Signatures\", numHashTables=5)\n",
    "\n",
    "# MinHash produces the signatures for the Characteritic matrix \n",
    "# numvHashTables is the number of the hash functioms that we want to use and the lenght of the signature \n",
    "model = minhash.fit(char_matrix)\n",
    "signatures = model.transform(char_matrix)\n",
    "\n",
    "# signatures.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Similar Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approxSimilarityJoin uses LSH automatically to find rows that it is most likely \n",
    "# to have same \"Signatures\"\n",
    "# threshold: pairs with Jaccard Distance lower than threshlod\n",
    "similar_pairs = model.approxSimilarityJoin(signatures, signatures, threshold=0.01, distCol=\"Jaccard Distance\")\n",
    "# similar_pairs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_pairs = similar_pairs.select(\"datasetA.Process ID\", \"datasetB.Process ID\", \n",
    "                     \"Jaccard Distance\") \\\n",
    "                    # .filter((col(\"datasetA.Process ID\") != col(\"datasetB.Process ID\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = [\"Process ID A\", \"Process ID B\", \"Jaccard Distance\"]\n",
    "similar_pairs = similar_pairs.toDF(*new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar_pairs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = similar_pairs.join(grouped_df, similar_pairs[\"Process ID A\"] == col(\"Process ID\")) \\\n",
    "                     .select(col(\"Process ID A\"), col(\"Process ID B\"), col(\"Log\").alias(\"Log A\")) \\\n",
    "                     .join(grouped_df, similar_pairs[\"Process ID B\"] == col(\"Process ID\")) \\\n",
    "                     .select(col(\"Process ID A\"), col(\"Process ID B\"), col(\"Log A\"), col(\"Log\").alias(\"Log B\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check using Original Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_check(x,y):\n",
    "    return x==y\n",
    "\n",
    "original_checking = udf(original_check, BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_pairs = pairs.filter(original_checking(col(\"Log A\"), col(\"Log B\")))\n",
    "same_pairs = same_pairs.groupBy(\"Log A\").agg(collect_set(\"Process ID A\").alias(\"Process Set\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Log A: array (nullable = false)\n",
      " |    |-- element: array (containsNull = false)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- Process Set: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "same_pairs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part1Observations.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_pairs_explode = same_pairs.select(same_pairs[\"Log A\"], same_pairs[\"Process Set\"], explode(same_pairs[\"Process Set\"]).alias(\"Process ID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_group(process_set):\n",
    "    process_set_string = ', '.join(str(x) for x in process_set)\n",
    "    return f\"Group: {{{process_set_string}}}\"\n",
    "\n",
    "def format_log(log, process_id):\n",
    "    log_formatted = \"\"\n",
    "    for l in log:\n",
    "        log_concat = ', '.join(str(x) for x in l)\n",
    "        log_formatted += f\"<{log_concat}, {process_id}>\\n\"\n",
    "    return log_formatted\n",
    "\n",
    "def format_group_logs(group, logs):\n",
    "    formatted = f\"{group}\\n\\n\" + \"\\n\".join(logs) \n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDFs for Formatting Output - part1Observations.txt\n",
    "format_group_udf = udf(format_group, StringType())\n",
    "format_udf = udf(format_log, StringType())\n",
    "final_format_udf = udf(format_group_logs, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_group = same_pairs_explode.withColumn(\"Group\", format_group_udf(col(\"Process Set\")))\n",
    "formatted_df = formatted_group.withColumn(\"Formatted Log\", format_udf(col(\"Log A\"), col(\"Process ID\")))\n",
    "grouped_logs = formatted_df.groupBy(\"Group\").agg(collect_list(\"Formatted Log\").alias(\"Group Log\"))\n",
    "final_formatted = grouped_logs.withColumn(\"Formatted\", final_format_udf(col(\"Group\"), col(\"Group Log\"))).select(\"Formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_formatted.coalesce(partition).write.mode('overwrite').text('part1Observations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'part1Observations/.': Is a directory\n",
      "rm: cannot remove 'part1Observations/..': Is a directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='rmdir part1Observations', returncode=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(\"mkdir -p output && cat part1Observations/part-* > output/part1Observations.txt\", shell=True)\n",
    "subprocess.run(\"find part1Observations/ -name 'part-*' -delete\", shell=True)\n",
    "subprocess.run(\"rm -f part1Observations/.*\", shell=True)\n",
    "subprocess.run(\"rm -f part1Observations/_SUCCESS\", shell=True) \n",
    "subprocess.run(\"rmdir part1Observations\", shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part1Output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df = df.withColumn('pid_integer', df['Process ID'].cast(IntegerType()))\n",
    "max_process_id = df.agg({\"pid_integer\": \"max\"}).collect()[0][0]\n",
    "# print(max_process_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_log_output(log):\n",
    "    epoch = int(time.time())\n",
    "    rand = random.randint(1000, 9999)\n",
    "    process_id = f\"{epoch}{rand}\"\n",
    "    log_formatted = \"\"\n",
    "    for l in log:\n",
    "        log_concat = ', '.join(str(x) for x in l)\n",
    "        log_formatted += f\"<{log_concat}, {process_id}>\\n\"\n",
    "\n",
    "    formatted = f\"{process_id}:\\n\" + log_formatted\n",
    "    return formatted\n",
    "\n",
    "format_udf_output = udf(format_log_output, StringType())\n",
    "formatted_df_output = same_pairs.withColumn(\"Formatted\", format_udf_output(col(\"Log A\"))).select(\"Formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_df_output.coalesce(partition).write.mode('overwrite').text('part1Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'part1Output/.': Is a directory\n",
      "rm: cannot remove 'part1Output/..': Is a directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='rmdir part1Output', returncode=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(\"cat part1Output/part-* > output/part1Output.txt\", shell=True)\n",
    "subprocess.run(\"find part1Output/ -name 'part-*' -delete\", shell=True)\n",
    "subprocess.run(\"rm -f part1Output/.*\", shell=True)\n",
    "subprocess.run(\"rm -f part1Output/_SUCCESS\", shell=True) \n",
    "subprocess.run(\"rmdir part1Output\", shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another aproach - Shingling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_shingling(text, k):\n",
    "    shingles = set()\n",
    "    for i in range(len(text) - k + 1):\n",
    "        shingle = text[i:i + k]\n",
    "        shingles.add(shingle)\n",
    "    return list(shingles)\n",
    "\n",
    "k_shingling_udf = udf(lambda text: k_shingling(text, 5), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(\"data.txt\").toDF(\"Log\")\n",
    "log = udf(lambda x: x[1:-4], StringType()) \n",
    "df = df.withColumn('Log_split', log('Log')) \n",
    "df.collect()\n",
    "df_shingles = df.withColumn(\"Shingles\", k_shingling_udf(df[\"Log_split\"]))\n",
    "# df_shingles.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Log: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- Log_split: string (nullable = true)\n",
      " |-- Shingles: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "First Server\n",
      "Second Server\n",
      "Communication Type\n",
      "Process ID\n",
      "root\n",
      " |-- Log: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- Log_split: string (nullable = true)\n",
      " |-- Shingles: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- First Server: string (nullable = true)\n",
      " |-- Second Server: string (nullable = true)\n",
      " |-- Communication Type: string (nullable = true)\n",
      " |-- Process ID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_shingles = df_shingles.withColumn(\"Log\", regexp_replace(col(\"Log\"), \"[<>]\", \"\"))\n",
    "df_shingles = df_shingles.withColumn(\"Log\", regexp_replace(col(\"Log\"), \",\", \"\"))\n",
    "df_shingles = df_shingles.withColumn(\"Log\", split(col(\"Log\"), \" \"))\n",
    "\n",
    "df_shingles.printSchema()\n",
    "# df_shingles.show()\n",
    "\n",
    "columns = [\"First Server\", \"Second Server\", \"Communication Type\", \"Process ID\"]\n",
    "\n",
    "for i in range(len(columns)):\n",
    "    print(columns[i])\n",
    "    df_shingles = df_shingles.withColumn(columns[i], col(\"Log\")[i])\n",
    "\n",
    "df_shingles.printSchema()\n",
    "# df_shingles.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = udf(lambda x: x[:-1], ArrayType(StringType())) \n",
    "df_shingles = df_shingles.withColumn('Log', log('Log')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Process ID: string (nullable = true)\n",
      " |-- Shingles: array (nullable = false)\n",
      " |    |-- element: array (containsNull = false)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- Log: array (nullable = false)\n",
      " |    |-- element: array (containsNull = false)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- Flat shingles: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_df = df_shingles.groupBy(\"Process ID\").agg(collect_set(\"Shingles\").alias(\"Shingles\"),collect_list(\"Log\").alias(\"Log\"))\n",
    "grouped_df = grouped_df.withColumn(\"Flat shingles\", flatten(col(\"Shingles\")))\n",
    "grouped_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Process ID: string (nullable = true)\n",
      " |-- Characteristic Matrix: vector (nullable = true)\n",
      "\n",
      "Rows of Characteristic Matrix:  ['eques', 'Respo', ', Res', 'Reque', ' Resp', 'quest', 'spons', 'ponse', ', Req', ' Requ', 'espon', ', S-1', '3, Re', '.3, R', ' S-1.', 'null,', 'ull, ', '3, S-', '.3, S', 'onse,', 'uest,', '1.3, ', 'S-1.3', '-1.3,', '.2, R', '1.1, ', '2, Re', '1.2, ', 'S-1.1', '-1.1,', '1, Re', '.1, R', '-1.2,', 'S-1.2', '2, S-', '.2, S', ', S-2', '.1, S', '1, S-', ' null', 'l, Re', 'll, S', 'll, R', ', nul', 'l, S-', ', S-4', ', S-3', '0.3, ', '.4, R', '4, Re', '.4, S', '5.3, ', 'S-20.', '4, S-', '44.4,', 'S-40.', '4.4, ', 'S-35.', '2, nu', '-40.3', '9.3, ', '-44.4', 'S-32.', 'S-23.', 'S-44.', '40.3,', '.2, n', '5.1, ', '.3, n', '3, nu', '29.3,', '48.2,', '20.1,', '2.3, ', 'S-36.', 'S-29.', '6.4, ', '35.3,', '-29.3', '-48.2', 'S-48.', '0.1, ', '4.3, ', 'S-41.', '2.2, ', 'S-19.', '8.2, ', '9.4, ', '19.4,', '-35.3', '-20.1', '-19.4', '1, nu', ' S-20', '.1, n', '8.3, ', ' S-32', '-28.3', '17.2,', 'S-11.', 'S-28.', '-24.3', '7.2, ', 'S-17.', '-41.1', ' S-44', 'S-26.', 'S-24.', '36.2,', '12.2,', 'S-12.', '6.2, ', '23.2,', 'S-5.1', '41.1,', '-36.2', '-11.2', '-26.4', ' S-35', '-23.2', '24.3,', ' S-23', '-32.3', '32.3,', '-5.1,', '-17.2', '26.4,', 'S-27.', '-12.2', '28.3,', '3.2, ', '11.2,', ' S-40', ' S-48', ' S-19', ' S-41', ' S-29', ' S-36', '6.3, ', '-35.1', ', S-5', '-9.3,', '-46.4', ' S-11', '0.2, ', 'S-9.3', '-23.1', '4.2, ', ' S-26', '32.2,', ' S-5.', '46.4,', '9.2, ', 'S-9.2', '7.3, ', 'S-25.', '-20.2', '-20.3', '-2.3,', '-27.1', '20.2,', '7.1, ', '3.3, ', '38.1,', 'S-38.', '-41.2', 'S-31.', '27.1,', '27.3,', '41.2,', 'S-4.3', '23.1,', '-38.1', 'S-14.', '32.1,', 'S-45.', ', S-9', ' S-28', '14.2,', '-32.1', ' S-9.', ' S-27', '-9.2,', '25.1,', '36.3,', 'S-46.', '-23.3', '-15.3', '-27.3', '8.1, ', '20.3,', '15.3,', '-31.3', '-45.3', '-25.1', '45.3,', '-36.3', 'S-15.', '31.3,', 'S-2.3', ' S-17', '-30.3', '-4.3,', '35.1,', '30.3,', '23.3,', ' S-12', 'S-30.', ' S-24', '2.1, ', '-32.2', '-14.2', '3.1, ', ' S-46', ' S-15', ' S-2.', ' S-31', ' S-25', ' S-4.', ' S-30', ' S-14', ' S-38', ' S-45']\n"
     ]
    }
   ],
   "source": [
    "characteristics = CountVectorizer(inputCol=\"Flat shingles\", outputCol=\"Characteristic Matrix\")\n",
    "\n",
    "model = characteristics.fit(grouped_df)\n",
    "char_matrix = model.transform(grouped_df).select(\"Process ID\", \"Characteristic Matrix\")\n",
    "\n",
    "char_matrix.printSchema()\n",
    "# char_matrix.show()\n",
    "\n",
    "shingles = model.vocabulary\n",
    "print(\"Rows of Characteristic Matrix: \", shingles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "minhash = MinHashLSH(inputCol=\"Characteristic Matrix\", outputCol=\"Signatures\", numHashTables=5)\n",
    "\n",
    "# MinHash produces the signatures for the Characteritic matrix \n",
    "# numvHashTables is the number of the hush functioms that we want to use and the lenght of the signature \n",
    "model = minhash.fit(char_matrix)\n",
    "signatures = model.transform(char_matrix)\n",
    "\n",
    "# signatures.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approxSimilarityJoin uses autmatically LSH to find rows that it is most likely \n",
    "# to have same \"Signatures\"\n",
    "# threshold: pairs with Jaccard Distance lower than threshlod\n",
    "similar_pairs = model.approxSimilarityJoin(signatures, signatures, threshold=0.2, distCol=\"Jaccard Distance\")\n",
    "# similar_pairs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_pairs = similar_pairs.select(\"datasetA.Process ID\", \"datasetB.Process ID\", \n",
    "                     \"Jaccard Distance\")\\\n",
    "                        # .filter((col(\"datasetA.Process ID\") != col(\"datasetB.Process ID\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = [\"Process ID A\", \"Process ID B\", \"Jaccard Distance\"]\n",
    "similar_pairs = similar_pairs.toDF(*new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = similar_pairs.join(grouped_df, similar_pairs[\"Process ID A\"] == col(\"Process ID\")) \\\n",
    "                     .select(col(\"Process ID A\"), col(\"Process ID B\"), col(\"Log\").alias(\"Log A\")) \\\n",
    "                     .join(grouped_df, similar_pairs[\"Process ID B\"] == col(\"Process ID\")) \\\n",
    "                     .select(col(\"Process ID A\"), col(\"Process ID B\"), col(\"Log A\"), col(\"Log\").alias(\"Log B\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_check(x,y):\n",
    "    return x==y\n",
    "\n",
    "orifinal_checking = udf(original_check, BooleanType())\n",
    "same_pairs = pairs.filter(orifinal_checking(col(\"Log A\"), col(\"Log B\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/25 23:55:24 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "same_pairs = same_pairs.groupBy(\"Log A\").agg(collect_set(\"Process ID A\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_intensive_systems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
