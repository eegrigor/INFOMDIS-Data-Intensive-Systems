{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from pyspark.sql.functions import split, col, regexp_replace, collect_list, explode, concat, collect_set, array_union, flatten\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, first, count\n",
    "from pyspark.sql.types import ArrayType, StringType, BooleanType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans, BisectingKMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/27 00:15:32 WARN Utils: Your hostname, abha-ThinkPad-P14s-Gen-4 resolves to a loopback address: 127.0.1.1; using 192.168.178.94 instead (on interface wlp2s0)\n",
      "24/06/27 00:15:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/27 00:15:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/27 00:15:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.178.94:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Projet-Task-2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7313a95e2750>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Projet-Task-2\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"14G\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"10G\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"2G\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:MaxGCPauseMillis=500 -XX:InitiatingHeapOccupancyPercent=35\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC -XX:MaxGCPauseMillis=500 -XX:InitiatingHeapOccupancyPercent=35\") \\\n",
    "    .config(\"spark.sql.codegen.wholeStage\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "# spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", partition)\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 100)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(\"output/part1Output.txt\").toDF(\"Log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Log: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- First Server: string (nullable = true)\n",
      " |-- Second Server: string (nullable = true)\n",
      " |-- Communication Type: string (nullable = true)\n",
      " |-- Process ID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Log\", regexp_replace(col(\"Log\"), r\"^\\d+:\\n\", \"\"))\n",
    "df = df.withColumn(\"Log\", regexp_replace(col(\"Log\"), r\", \\d+\\n\", \"\\n\"))\n",
    "df = df.withColumn(\"Log\", regexp_replace(col(\"Log\"), \"[<>]\", \"\"))\n",
    "df = df.withColumn(\"Log\", regexp_replace(col(\"Log\"), \",\", \"\"))\n",
    "df = df.withColumn(\"Log\", split(col(\"Log\"), \" \"))\n",
    "\n",
    "# df.printSchema()\n",
    "# df.show()\n",
    "\n",
    "columns = [\"First Server\", \"Second Server\", \"Communication Type\", \"Process ID\"]\n",
    "\n",
    "for i in range(len(columns)):\n",
    "    # print(columns[i])\n",
    "    df = df.withColumn(columns[i], col(\"Log\")[i])\n",
    "\n",
    "df = df.filter(col(\"Process ID\").isNotNull())\n",
    "\n",
    "df.printSchema()\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Process ID from request \n",
    "log = udf(lambda x: x[:-1], ArrayType(StringType())) \n",
    "df = df.withColumn('Refined Log', log('Log')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------------+------------------+--------------+--------------------+\n",
      "|                 Log|First Server|Second Server|Communication Type|    Process ID|         Refined Log|\n",
      "+--------------------+------------+-------------+------------------+--------------+--------------------+\n",
      "|[null, S-3, Reque...|        null|          S-3|           Request|17194363075607|[null, S-3, Request]|\n",
      "|[S-3, S-9.1, Requ...|         S-3|        S-9.1|           Request|17194363075607|[S-3, S-9.1, Requ...|\n",
      "|[S-9.1, S-12, Req...|       S-9.1|         S-12|           Request|17194363075607|[S-9.1, S-12, Req...|\n",
      "|[S-12, S-14, Requ...|        S-12|         S-14|           Request|17194363075607|[S-12, S-14, Requ...|\n",
      "|[S-14, S-17, Requ...|        S-14|         S-17|           Request|17194363075607|[S-14, S-17, Requ...|\n",
      "|[S-17, S-14, Resp...|        S-17|         S-14|          Response|17194363075607|[S-17, S-14, Resp...|\n",
      "|[S-14, S-12, Resp...|        S-14|         S-12|          Response|17194363075607|[S-14, S-12, Resp...|\n",
      "|[S-12, S-9.1, Res...|        S-12|        S-9.1|          Response|17194363075607|[S-12, S-9.1, Res...|\n",
      "|[S-9.1, S-3, Resp...|       S-9.1|          S-3|          Response|17194363075607|[S-9.1, S-3, Resp...|\n",
      "|[S-3, null, Respo...|         S-3|         null|          Response|17194363075607|[S-3, null, Respo...|\n",
      "|[null, S-3, Reque...|        null|          S-3|           Request|17194363075364|[null, S-3, Request]|\n",
      "|[S-3, S-9.1, Requ...|         S-3|        S-9.1|           Request|17194363075364|[S-3, S-9.1, Requ...|\n",
      "|[S-9.1, S-18, Req...|       S-9.1|         S-18|           Request|17194363075364|[S-9.1, S-18, Req...|\n",
      "|[S-18, S-9.1, Res...|        S-18|        S-9.1|          Response|17194363075364|[S-18, S-9.1, Res...|\n",
      "|[S-9.1, S-12, Req...|       S-9.1|         S-12|           Request|17194363075364|[S-9.1, S-12, Req...|\n",
      "|[S-12, S-1, Reque...|        S-12|          S-1|           Request|17194363075364|[S-12, S-1, Request]|\n",
      "|[S-1, S-12, Respo...|         S-1|         S-12|          Response|17194363075364|[S-1, S-12, Respo...|\n",
      "|[S-12, S-9.1, Res...|        S-12|        S-9.1|          Response|17194363075364|[S-12, S-9.1, Res...|\n",
      "|[S-9.1, S-3, Resp...|       S-9.1|          S-3|          Response|17194363075364|[S-9.1, S-3, Resp...|\n",
      "|[S-3, null, Respo...|         S-3|         null|          Response|17194363075364|[S-3, null, Respo...|\n",
      "+--------------------+------------+-------------+------------------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Process ID: string (nullable = true)\n",
      " |-- Refined Log: array (nullable = false)\n",
      " |    |-- element: array (containsNull = false)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- First Server: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- Second Server: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- FCommunication Type: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- Log: array (nullable = false)\n",
      " |    |-- element: array (containsNull = false)\n",
      " |    |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_df = df.groupBy(\"Process ID\").agg(collect_list(\"Refined Log\").alias(\"Refined Log\"), \n",
    "                                          collect_list(\"First Server\").alias(\"First Server\"),\n",
    "                                          collect_list(\"Second Server\").alias(\"Second Server\"),\n",
    "                                          collect_list(\"Communication Type\").alias(\"FCommunication Type\"),\n",
    "                                          collect_list(\"Log\").alias(\"Log\"))\n",
    "\n",
    "grouped_df.printSchema()\n",
    "# grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Process ID: string (nullable = true)\n",
      " |-- First Server: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- Second Server: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- Servers: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distinct_servers_df = df.groupBy(\"Process ID\").agg(collect_set(\"First Server\").alias(\"First Server\"),\n",
    "                                                   collect_set(\"Second Server\").alias(\"Second Server\"))\n",
    "\n",
    "distinct_servers_df = distinct_servers_df.withColumn(\"Servers\", array_union(\"First Server\", \"Second Server\"))\n",
    "\n",
    "distinct_servers_df.printSchema()\n",
    "# distinct_servers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Process ID: string (nullable = true)\n",
      " |-- Characteristic Matrix: vector (nullable = true)\n",
      "\n",
      "Rows of Characteristic Matrix:  ['S-3', 'null', 'S-5', 'S-8', 'S-9.2', 'S-9.1', 'S-18', 'S-12', 'S-17', 'S-4', 'S-1', 'S-2', 'S-13', 'S-14', 'S-16', 'S-6', 'S-15', 'S-10.2', 'S-10.1', 'S-11', 'S-19.1', 'S-19.2', 'S-7']\n"
     ]
    }
   ],
   "source": [
    "characteristics = CountVectorizer(inputCol=\"Servers\", outputCol=\"Characteristic Matrix\")\n",
    "\n",
    "model = characteristics.fit(distinct_servers_df)\n",
    "char_matrix = model.transform(distinct_servers_df).select(\"Process ID\", \"Characteristic Matrix\")\n",
    "\n",
    "char_matrix.printSchema()\n",
    "# char_matrix.show()\n",
    "\n",
    "servers = model.vocabulary\n",
    "print(\"Rows of Characteristic Matrix: \", servers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "minhash = MinHashLSH(inputCol=\"Characteristic Matrix\", outputCol=\"Signatures\", numHashTables=5)\n",
    "\n",
    "# MinHash produces the signatures for the Characteritic matrix \n",
    "# numvHashTables is the number of the hush functioms that we want to use and the lenght of the signature \n",
    "model = minhash.fit(char_matrix)\n",
    "signatures = model.transform(char_matrix)\n",
    "\n",
    "# signatures.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approxSimilarityJoin uses autmatically LSH to find rows that it is most likely \n",
    "# to have same \"Signatures\"\n",
    "# threshold: pairs with Jaccard Distance lower than threshlod\n",
    "similar_pairs = model.approxSimilarityJoin(signatures, signatures, threshold=0.4, distCol=\"Jaccard Distance\")\n",
    "# similar_pairs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_pairs = similar_pairs.select(\"datasetA.Process ID\", \"datasetB.Process ID\", \n",
    "                     \"Jaccard Distance\") \\\n",
    "                    # .filter((col(\"datasetA.Process ID\") != col(\"datasetB.Process ID\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = [\"Process ID A\", \"Process ID B\", \"Jaccard Distance\"]\n",
    "similar_pairs = similar_pairs.toDF(*new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Creating a dataframe with the pairwise similarity metric \n",
    "similarities = similar_pairs.groupBy(\"Process ID A\").pivot(\"Process ID B\")\\\n",
    "                                                .agg(first(\"Jaccard Distance\")).fillna(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with each Process ID and the corresponding feature vectors\n",
    "feature_columns = similarities.columns[1:]  \n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"Feature Vectors\")\n",
    "features = assembler.transform(similarities).select(\"Process ID A\", \"Feature Vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/27 00:15:42 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/06/27 00:15:43 WARN DAGScheduler: Broadcasting large task binary with size 1290.0 KiB\n",
      "24/06/27 00:15:46 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "24/06/27 00:15:47 WARN DAGScheduler: Broadcasting large task binary with size 1936.5 KiB\n",
      "24/06/27 00:15:48 WARN DAGScheduler: Broadcasting large task binary with size 1045.4 KiB\n",
      "24/06/27 00:15:54 WARN DAGScheduler: Broadcasting large task binary with size 1046.0 KiB\n",
      "24/06/27 00:15:54 WARN DAGScheduler: Broadcasting large task binary with size 1046.7 KiB\n",
      "24/06/27 00:15:54 WARN DAGScheduler: Broadcasting large task binary with size 1047.1 KiB\n",
      "24/06/27 00:15:54 WARN DAGScheduler: Broadcasting large task binary with size 1047.1 KiB\n",
      "24/06/27 00:15:54 WARN DAGScheduler: Broadcasting large task binary with size 1047.4 KiB\n",
      "24/06/27 00:15:54 WARN DAGScheduler: Broadcasting large task binary with size 1047.2 KiB\n",
      "24/06/27 00:15:55 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "24/06/27 00:15:55 WARN DAGScheduler: Broadcasting large task binary with size 1046.5 KiB\n",
      "24/06/27 00:15:55 WARN DAGScheduler: Broadcasting large task binary with size 1046.5 KiB\n",
      "24/06/27 00:15:55 WARN DAGScheduler: Broadcasting large task binary with size 1046.5 KiB\n",
      "24/06/27 00:15:55 WARN DAGScheduler: Broadcasting large task binary with size 1046.5 KiB\n",
      "24/06/27 00:15:55 WARN DAGScheduler: Broadcasting large task binary with size 1046.5 KiB\n",
      "24/06/27 00:15:56 WARN DAGScheduler: Broadcasting large task binary with size 1849.9 KiB\n",
      "24/06/27 00:16:00 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cluster_number = 50\n",
    "\n",
    "kmeans = KMeans(k=cluster_number, seed=33, featuresCol=\"Feature Vectors\", predictionCol=\"Cluster Number\")\n",
    "model = kmeans.fit(features)\n",
    "\n",
    "# bisecting_kmeans = BisectingKMeans(k=cluster_number, seed=33, featuresCol=\"Feature Vectors\", predictionCol=\"Cluster Number\")\n",
    "# model = bisecting_kmeans.fit(features)\n",
    "\n",
    "predictions = model.transform(features)\n",
    "\n",
    "# predictions.select(\"Process ID A\",\"Cluster Number\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/27 00:16:00 WARN DAGScheduler: Broadcasting large task binary with size 1608.0 KiB\n",
      "24/06/27 00:16:04 WARN DAGScheduler: Broadcasting large task binary with size 1859.5 KiB\n",
      "[Stage 83:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score:  0.3475636226975157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/27 00:16:07 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator = ClusteringEvaluator(featuresCol=\"Feature Vectors\", predictionCol=\"Cluster Number\")\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions, {evaluator.metricName: \"silhouette\"})\n",
    "print(\"Silhouette score: \", silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.join(grouped_df, predictions[\"Process ID A\"] == col(\"Process ID\"))\\\n",
    "                            .select(\"Process ID\", \"Refined Log\", \"Log\",\"Cluster Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = predictions.groupBy(\"Cluster Number\").agg(collect_set(\"Process ID\").alias(\"Process ID\"),\n",
    "                                                     collect_set(\"Refined Log\").alias(\"Refined Log\"),\n",
    "                                                     collect_set(\"Log\").alias(\"Log\"), \n",
    "                                                     count(\"Process ID\").alias(\"Members Count\")) \\\n",
    "                                                     .orderBy(col(\"Members Count\").desc())\n",
    "\n",
    "# clusters.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/27 00:35:30 WARN DAGScheduler: Broadcasting large task binary with size 1866.2 KiB\n",
      "24/06/27 00:35:34 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+-------------+\n",
      "|Cluster Number|          Process ID|         Refined Log|                 Log|Members Count|\n",
      "+--------------+--------------------+--------------------+--------------------+-------------+\n",
      "|            48|[17194363074062, ...|[[[null, S-3, Req...|[[[null, S-3, Req...|          126|\n",
      "|             5|[17194363071246, ...|[[[null, S-3, Req...|[[[null, S-3, Req...|           81|\n",
      "|            33|[17194363073385, ...|[[[null, S-3, Req...|[[[null, S-3, Req...|           61|\n",
      "|            41|[17194363071426, ...|[[[null, S-3, Req...|[[[null, S-3, Req...|           61|\n",
      "|            15|[17194363072690, ...|[[[null, S-3, Req...|[[[null, S-3, Req...|           48|\n",
      "|            29|[17194363071846, ...|[[[null, S-3, Req...|[[[null, S-3, Req...|           48|\n",
      "|            34|[17194363077324, ...|[[[null, S-3, Req...|[[[null, S-3, Req...|           43|\n",
      "|            14|[17194363079516, ...|[[[null, S-3, Req...|[[[null, S-3, Req...|           42|\n",
      "|             2|[17194363071590, ...|[[[null, S-3, Req...|[[[null, S-3, Req...|           42|\n",
      "|            45|[17194363072858, ...|[[[null, S-3, Req...|[[[null, S-3, Req...|           41|\n",
      "|             6|[17194363079557, ...|[[[null, S-3, Req...|[[[null, S-3, Req...|           39|\n",
      "|            24|[17194363074995, ...|[[[null, S-3, Req...|[[[null, S-3, Req...|           39|\n",
      "|            22|[17194363076144, ...|[[[null, S-3, Req...|[[[null, S-3, Req...|           39|\n",
      "|            10|[17194363077988, ...|[[[null, S-3, Req...|[[[null, S-3, Req...|           39|\n",
      "|            17|[17194363078783, ...|[[[null, S-3, Req...|[[[null, S-3, Req...|           37|\n",
      "|             0|[17194363072297, ...|[[[null, S-3, Req...|[[[null, S-3, Req...|           36|\n",
      "|            35|[17194363077319, ...|[[[null, S-3, Req...|[[[null, S-3, Req...|           34|\n",
      "|            19|[17194363075972, ...|[[[null, S-3, Req...|[[[null, S-3, Req...|           32|\n",
      "|             8|[17194363078276, ...|[[[null, S-3, Req...|[[[null, S-3, Req...|           30|\n",
      "|            44|[17194363077193, ...|[[[null, S-3, Req...|[[[null, S-3, Req...|           30|\n",
      "+--------------+--------------------+--------------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clusters.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_group(process_set):\n",
    "    process_set_string = ', '.join(str(x) for x in process_set)\n",
    "    return f\"Group: {{{process_set_string}}}\"\n",
    "\n",
    "def format_log(logs):\n",
    "    log_formatted = \"\"\n",
    "    for log in logs:\n",
    "        log_concat = '\\n'.join(str(x) for x in log)\n",
    "        log_formatted += f\"{log_concat}\\n\\n\"\n",
    "    return log_formatted\n",
    "\n",
    "def format_group_logs(group, logs):\n",
    "    formatted = f\"{group}\\n\\n\" + \"\\n\".join(logs) \n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDFs for Formatting Output - part1Observations.txt\n",
    "format_group_udf = udf(format_group, StringType())\n",
    "format_udf = udf(format_log, StringType())\n",
    "final_format_udf = udf(format_group_logs, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_group = clusters.withColumn(\"Group\", format_group_udf(col(\"Process ID\")))\n",
    "formatted_df = formatted_group.withColumn(\"Formatted Log\", format_udf(col(\"Log\")))\n",
    "grouped_logs = formatted_df.groupBy(\"Group\").agg(collect_list(\"Formatted Log\").alias(\"Group Log\"))\n",
    "final_formatted = grouped_logs.withColumn(\"Formatted\", final_format_udf(col(\"Group\"), col(\"Group Log\"))).select(\"Formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/27 00:53:00 WARN DAGScheduler: Broadcasting large task binary with size 1861.7 KiB\n",
      "24/06/27 00:53:03 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/06/27 00:53:03 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/06/27 00:53:03 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/06/27 00:53:03 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n"
     ]
    }
   ],
   "source": [
    "final_formatted.coalesce(partition).write.mode('overwrite').text('part2Observations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'part2Observations/.': Is a directory\n",
      "rm: cannot remove 'part2Observations/..': Is a directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='rmdir part2Observations', returncode=0)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(\"mkdir -p output && cat part2Observations/part-* > output/part2Observations.txt\", shell=True)\n",
    "subprocess.run(\"find part2Observations/ -name 'part-*' -delete\", shell=True)\n",
    "subprocess.run(\"rm -f part2Observations/.*\", shell=True)\n",
    "subprocess.run(\"rm -f part2Observations/_SUCCESS\", shell=True) \n",
    "subprocess.run(\"rmdir part2Observations\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters = clusters.select(\"Log\")\n",
    "# # clusters = clusters.withColumn(\"Cluster Number\", col(\"Cluster Number\").cast(\"string\"))\n",
    "# # clusters = clusters.withColumn(\"Process ID\", col(\"Process ID\").cast(\"string\"))\n",
    "# clusters = clusters.withColumn(\"Log\", col(\"Log\").cast(\"string\"))\n",
    "# clusters.write.mode('overwrite').text(\"task2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Cluster Number: integer (nullable = false)\n",
      " |-- Process ID: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- Refined Log: array (nullable = false)\n",
      " |    |-- element: array (containsNull = false)\n",
      " |    |    |-- element: array (containsNull = false)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |-- Log: array (nullable = false)\n",
      " |    |-- element: array (containsNull = false)\n",
      " |    |    |-- element: array (containsNull = false)\n",
      " |    |    |    |-- element: string (containsNull = false)\n",
      " |-- Members Count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clusters.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_intensive_systems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
